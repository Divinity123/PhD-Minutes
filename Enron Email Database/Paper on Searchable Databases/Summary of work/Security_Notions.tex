In this section, we consider several experiment-based adversary on searchable encryption. We motivate with an experiment that is very similar to an IND-CPA experiment (for brevity, we do not give definition of standard IND-CPA here) specifically for keyword guessing attack. In the experiment, the adversary receives the encryption of the database he chooses, and his goal is to match keywords to their encryptions. To move a step further, we modify this experiment by only giving the adversary leakage of the encryption scheme associated to the database in the experiment. We then generalise the experiments to allow for arbitrary target functions of the adversary.


\subsection{Security Notion for Keyword Guessing Attack}
There is a multitude of ways to evaluate how well the adversary has recovered information through keyword guessing attack. We will present with one of the simplest here. Namely, the adversary wins if he recovers a correct pair of keyword and its encryption (with one guess).

For simplicity, we only consider snapshot attack here, so it suffices to assume that the searchable encryption scheme has some key generation function $\kgen(\cdot)$, some encryption function $\enc(\cdot)$ that encrypts the database, and there is a decryption function $\dec(\cdot)$ that decrypts encrypted documents one at a time. Because we are interested in security of the search index, we abuse $\dec(\cdot)$ to be the function that decrypts an encrypted document or a set of encrypted keywords associated to an encrypted document, and outputs the set of real keywords.

\begin{definition}[Security against Keyword Guessing Attack] \label{Security against Keyword Guessing Attack}
We define an keyword guessing attack adversary to be $\mathcal{A} = (\mathcal{A}_0, \mathcal{A}_1)$, where $\mathcal{A}_0$ is a probabilistic polynomial time (PPT) adversary which takes in the security parameter and generates a database $\db$, and $\mathcal{A}_1$ is a PPT adversary which takes in the security parameter, generated database $\db$ and encrypted database $\edb$, and returns a pair of keyword and encrypted keyword. The experiment can be described as follows:

\begin{pchstack}[center]
	\procedure{Exp$_{\adv}^{\text{Keyword-Guessing}}(n)$}{%
		\pcln \key \sample \kgen(\secparam) \\
		\pcln \db  \gets \mathcal{A}_0(\secparam) \\
		\pcln \edb \gets \enc(\secparam, \key, \db) \\
		\pcln (m, c) \gets \mathcal{A}_1(\secparam, \db, \edb) \\
		\pcln \pcif (\dec(c) = m) \pcreturn 1 - f(m, \db) \\
		\pcln \pcelse \pcreturn - f(m, \db)
	}
\end{pchstack}
where $f(m, \db)$ is a function that returns the frequency of keyword $m$ in database $\db$.

Advantage $\advantage_{\mathcal{A}}^{\text{Keyword-Guessing}}(n)$ of an inference attack adversary $\mathcal{A}$ is defined to be
\begin{equation}
	\advantage_{\mathcal{A}}^{\text{Keyword-Guessing}}(n) = \expect{\text{Exp}_{\adv}^{\text{Keyword-Guessing}}(n)}.
\end{equation}
We say that a scheme is secure against keyword guessing attack if $\advantage_{\mathcal{A}}^{\text{Keyword-Guessing}}(n)$ is less than some negligible function in $n$.
\end{definition}


Intuitively, a scheme is secure against keyword guessing attack if there is no way he can guess any plaintext-ciphertext pair with probability greater than the frequency of the plaintexts. There are other possible definitions, for instance, one can look at the probability the adversary guessing all pairs of plaintexts and ciphertexts correctly.

This security notion is hard to work with because the database the adversary can generate is literally anything and the encryption scheme can generate any ciphertext. But in our game, we really do not care if keywords are `1' and `2' or `one' and `two', nor do we care how the ciphertexts look like. To simplify the problem, we want to work with idealised databases and encryption schemes which allows us to ignore information that are not relevant to our security game. Due to necessity of efficient searchability, encryption schemes for searchable encryption cannot achieve IND-CPA security, and the `insecure part' of encryption function (and query functions) are understood in terms of leakage functions. Informally speaking, leakage function associated to a scheme captures what is leaked about the unencrypted database by looking at its encryption. For example, number of documents is part of leakage for any scheme that does not pad fake documents because after enough queries, all documents will be returned at least once (unless part of the database is never going to be returned by any query, but that defeats the purpose of putting them there in the first place).

In the next experiment, the adversary is only given the leakage associated to the mechanism applied to the generated database. We argue that by choosing the leakage carefully, the advantage for the experiment will be the same as the previous one, up to an additive negligible term. We begin by defining leakage of searchable encryption.


\begin{definition}[Leakage of Searchable Encryption]
Let $\enc$ be the encryption algorithm for some searchable encryption scheme. We say $\leak_{\mathcal{M}}$ is a valid leakage function for the mechanism if there exists an adversary $\mathcal{A}$ and a simulator $\mathcal{S}$ such that for any PPT distinguishers $\mathcal{D}$, the following two games are indistinguishable.
	
\begin{pchstack}[center]
\procedure{Real$_{\mathcal{A}}(n)$}{%
	\pcln \key \sample \kgen(\secparam) \\
	\pcln \db  \gets \mathcal{A}(\secparam) \\
	\pcln \edb \gets \enc(\secparam, \key, \db) \\
	\pcln \pcreturn \edb
}	

\pchspace
\procedure{Sim$_{\mathcal{A,S}}(n)$}{%
	\pcln \key  \sample \kgen(\secparam) \\
	\pcln \db   \gets \mathcal{A}(\secparam) \\
	\pcln \leak \gets \leak_{\mathcal{M}}(\secparam, \key, \db) \\
	\pcln \edb  \gets \mathcal{S}(\secparam, \leak) \\
	\pcln \pcreturn \edb
}
\end{pchstack}
\end{definition}

This definition quantifies a class of valid leakages. But clearly not all of them are useful. For instance, the identity function is always a valid leakage in this definition. What we are interested in is really the minimal leakage which makes these two games indistinguishable. (\textbf{TO DO: 1. Define minimal leakage properly. 2.Discuss the implication when the leakage used in the experiment is not minimal.})


\begin{definition}[Minimal Leakage]
	
\end{definition}

Now we are ready to define security against idealised keyword guessing attack.


\begin{definition}[Security against Idealised Keyword Guessing Attack]
We define an idealised keyword guessing attack adversary to be $\mathcal{A} = (\mathcal{A}_0, \mathcal{A}_1)$, where $\mathcal{A}_0$ is a PPT adversary which takes in the security parameter and generates a database $\db$, and $\mathcal{A}_1$ is any adversary (not necessarily polynomial time bounded) which takes in the security parameter, generated database $\db$ and leakage $\leak$ generated by the minimal leakage function $\leak_{\mathcal{M}}$, and returns a pair of keyword and encrypted keyword. The experiment can be described as follows:
\begin{pchstack}[center]
	\procedure{Exp$_{\adv}^{\text{Keyword-Guessing-Ideal}}(n)$}{%
		\pcln \key \sample \kgen(\secparam) \\
		\pcln \db  \gets \mathcal{A}_0(\secparam) \\
		\pcln \leak \gets \leak_{\mathcal{M}}(\secparam, \key, \db) \\
		\pcln (m, c) \gets \mathcal{A}_1(\secparam, \db, \leak) \\
		\pcln \pcif (\dec(c) = m) \ \pcreturn 1 - f(m, \db) \\
		\pcln \pcelse \pcreturn - f(m, \db)
	}
\end{pchstack}
where $f(m, \db)$ is a function that returns the frequency of keyword $m$ in database $\db$.

Advantage $\advantage_{\mathcal{A}}^{\text{Keyword-Guessing-Ideal}}(n)$ of an idealised inference attack adversary $\mathcal{A}$ is defined to be
\begin{equation}
\advantage_{\mathcal{A}}^{\text{Keyword-Guessing-Ideal}}(n) = \expect{\text{Exp}_{\adv}^{\text{Keyword-Guessing-Ideal}}(n)}.
\end{equation}
We say that a scheme is secure against idealised keyword guessing attack if $\advantage_{\mathcal{A}}^{\text{Keyword-Guessing-Ideal}}(n)$ is less than some negligible function in $n$.
\end{definition}

Notice that in the definition, $\mathcal{A}_1$ does not need to be PT bounded. This is because we are in the idealised world where the cryptographic part has already been dealt with in the leakage. Also, the decryption function $\dec(\cdot)$ we have in the experiment is idealised. For example, if an IND-DCPA secure deterministic encryption is used to encrypt keywords, then the idealised encryption function and decryption function are just bijections. This is different from original encryption and decryption functions because they are not keyed so it the adversary cannot use brute force to recover the key and break security of the scheme.

It is clear that if $\mathcal{L_M}$ is a well-defined minimal leakage, then all PPT distinguisher $\mathcal{D}$ can only distinguish the real experiment and the idealised experiment with probability no greater than a negligible function in $n$.



\subsection{Moving from Fixed Target Function to Generic Gain Function}
In the two games for keyword guessing attack we defined above, the content returned by the games are fixed so proving security with respect to the games only says the scheme is secure against this specific adversary who tries to recover this specific information. This is very inflexible as a security notion for application in searchable encryption. In addition, wrong guesses in these games are completely punished in this setting in the sense that they will never show up in the advantage term, even though wrong guesses can still reveal some information. Consider the case where the adversary always tries to guess decryption of some ciphertext $c$ and he always fails because his guess is some $m$ such that $\dec(c) \neq m$. But ultimately, the goal of the adversary can be to recover some keywords in some documents. If $m$ always appears together with $\dec(c)$ in all documents, then in fact, the adversary recovers a significant amount of information about the underlying database, even though the guess itself is wrong. So we need to reward those guesses that are not completely correct but still reveals information about the database appropriately.

To do so, we use idea of gain function from g-leakage paper \cite{6266165}. To put it in simple words, instead returning something from the experiment and analyse it in the advantage term, we compute the gain at the end of the experiment, and the advantage is defined to be the expectation of the result of the experiment for the best adversary. We now define the games with gain functions.


\begin{definition}[Real Game with Gain Function]
We define a real adversary with gain function $g$ to be $\mathcal{A} = (\mathcal{A}_0, \mathcal{A}_1)$, where $\mathcal{A_0}$ is a PPT adversary which takes in the security parameter and generates a database $\db$, and $\mathcal{A}_1$ is a PPT adversary which takes in the security parameter, generated database $\db$ and encrypted database $\edb$, and returns some guess $w$ in the set of allowed guesses $\mathcal{W}$. The experiment can be described as follows:
\begin{pchstack}[center]
\procedure{Real$_{\adv}^{\ g}(n)$}{%
	\pcln \key \sample \kgen(\secparam) \\
	\pcln \db  \gets \mathcal{A}_0(\secparam) \\
	\pcln \edb \gets \enc(\secparam, \key, \db) \\
	\pcln w \gets \mathcal{A}_1(\secparam, \db, \edb) \\
	\pcln \pcreturn g(w, (\db, \key), \edb)
}
\end{pchstack}

Advantage $\advantage_{\mathcal{A},g}^{\text{Real}}(n)$ is defined to be:
\begin{equation}
	\advantage_{\mathcal{A},g}^{\text{Real}}(n) = \expect{\text{Real}_{\adv}^{\ g}(n)}.
\end{equation}
\end{definition}


\begin{definition}[Ideal Game with Gain Function]
We define an ideal adversary with gain function $g$ to be $\mathcal{A} = (\mathcal{A}_0, \mathcal{A}_1)$, where $\mathcal{A_0}$ is a PPT adversary which takes in the security parameter and generates a database $\db$, and $\mathcal{A}_1$ is a PPT adversary which takes in the security parameter, generated database $\db$ and minimal leakage $\leak$, and returns some guess $w$ in the set of allowed guesses $\mathcal{W}$. The experiment can be described as follows:
\begin{pchstack}[center]
\procedure{Ideal$_{\adv}^{\ \mathcal{L}, g}(n)$}{%
	\pcln \key \sample \kgen(\secparam) \\
	\pcln \db  \gets \mathcal{A}_0(\secparam) \\
	\pcln \leak \gets \leak_{\mathcal{M}}(\secparam, \key, \db) \\
	\pcln w \gets \mathcal{A}_1(\secparam, \db, \leak) \\
	\pcln \pcreturn g(w, (\db, \key), \leak)
}
\end{pchstack}
	
Advantage $\advantage_{\mathcal{A}, \mathcal{L}, g}^{\text{Ideal}}(n)$ is defined to be:
\begin{equation}
	\advantage_{\mathcal{A}, \mathcal{L}, g}^{\text{Ideal}}(n) = \expect{\text{Ideal}_{\adv}^{\ \mathcal{L}, g}(n)}.
\end{equation}
\end{definition}

Note that we are not putting any condition to quantify when a scheme is secure under these notions, because the advantage really depends on the choice of $g$. But with well-defined $g$, the advantage will have an operational meaning and we can understand the level of security offered by our scheme from there.




\subsection{Games with Gain Function are equivalent to some Extended Posterior g-vulnerability}
In this subsection, we prove that in fact, we can look at the advantage from the games or some corresponding extended posterior g-vulnerability exchangeably. To do so, we need a slight tweak to the games above.

Let $\pi$ be the prior on the joint distribution of databases and keys. Then the games above is equivalent to the following.
\begin{pchstack}[center]
\procedure{Real$_{\adv}^{\ \pi,g}(n)$}{%
	\pcln (\db, \key) \sample \pi \\
	\pcln \edb \gets \enc(\secparam, \key, \db) \\
	\pcln w \gets \mathcal{A}_1(\secparam, \db, \edb) \\
	\pcln \pcreturn g(w, (\db, \key), \edb)
}

\pchspace
\procedure{Ideal$_{\adv}^{\ \pi, \mathcal{L},g}(n)$}{%
	\pcln (\db, \key) \sample \pi \\
	\pcln \leak \gets \leak_{\mathcal{M}}(\secparam, \key, \db) \\
	\pcln w \gets \mathcal{A}_1(\secparam, \db, \leak) \\
	\pcln \pcreturn g(w, (\db, \key), \leak)
}
\end{pchstack}


\begin{theorem}[Real game is equivalent to an extended posterior g-vulnerability]
Let $\pi$ of the extended posterior g-vulnerability be that in the real experiment. Let $\mathcal{X} = \{(\db, k)\}$. Let $\mathcal{Y} = \{\enc(\secparam, \key, \db)\}$. Let $\mathcal{C}_{\leak}$ be the channel matrix from $\mathcal{X}$ to $\mathcal{Y}$. Then
\begin{equation}
	\sup_{\pi \in \text{Dist}} \ \sup_{\mathcal{A}} \advantage_{\mathcal{A}, \pi, g}^{\text{Real}}(n) = EV_g(\pi, \mathcal{C}_{\leak}).
\end{equation}
where $Dist$ is the set of valid prior for the database. 
\end{theorem}



\begin{theorem}[Ideal game is equivalent to an extended posterior g-vulnerability]
	Let $\pi$ of the extended posterior g-vulnerability be that in the real experiment. Let $\mathcal{X} = \{(\db, k)\}$. Let $\mathcal{Y} = \{\leak_{\mathcal{M}}(\secparam, \key, \db)\}$. Let $\mathcal{C}_{\leak}$ be the channel matrix from $\mathcal{X}$ to $\mathcal{Y}$. Then
	\begin{equation}
	\sup_{\pi \in \text{Dist}} \ \inf_{\leak \in \text{Valid}} \ \sup_{\mathcal{A}} \advantage_{\mathcal{A}, \pi, \mathcal{L}, g}^{\text{Real}}(n) = EV_g(\pi, \mathcal{C}_{\leak}).
	\end{equation}
	where $Dist$ is the set of valid prior for the database. 
\end{theorem}
\textbf{Note: Need to explain the leakage in the theorem after we define minimal leakage properly.}

We prove the second theorem, and by taking the leakage function as the encryption function, and change $g$ appropriately, we obtain the first theorem.


\begin{proof} 
For simplicity, we assume $Dist$ is a point distribution. Furthermore, $\inf_{\leak \in \text{Valid}}$ is a technical condition to ensure the leakage we consider is really some minimal leakage, so it suffices to look at the expression $\sup_{\mathcal{A}} \advantage_{\mathcal{A}, \pi, \mathcal{L}, g}^{\text{Real}}(n)$ with $\mathcal{L}$ being a valid minimal leakage. Suppose $\mathcal{L}$ is indeed a valid minimal leakage, then

\begin{align}
  & \sup_{\mathcal{A}} \advantage_{\mathcal{A}, \pi, \mathcal{L}, g}^{\text{Real}}(n) \\
= &	\sup_{\mathcal{A}} \expect{\text{Ideal}_{\adv}^{\ \mathcal{L}, g}(n)} \label{thm-equiv-1} \\
= & \mathbb{E}_{\mathcal{L}} \left[ \max_{w \in {\mathcal{W}}} \mathbb{E}_{\pi} \left[ g(w, (\db, \key), \mathcal{L}) \mid \mathcal{L} \right] \right] \label{thm-equiv-2} \\
= &  \mathbb{E}_{\mathcal{Y}} \left[ \max_{w \in {\mathcal{W}}} \mathbb{E}_{\pi} \left[ g(w, \mathcal{X}, \mathcal{Y}) \mid \mathcal{Y} \right] \right] \\
= &  \sum_{y \in {\mathcal{Y}}} \prob{\mathcal{Y} = y} \max_{w \in {\mathcal{W}}} \sum_{x \in {\mathcal{X}}} \prob{\mathcal{X} = x \mid \mathcal{Y} = y} g(w, x, y) \\
= & \sum_{y \in {\mathcal{Y}}} \max_{w \in {\mathcal{W}}} \sum_{x \in {\mathcal{X}}} \prob{\mathcal{X} = x, \mathcal{Y} = y} g(w, x, y) \\
= & \sum_{y \in {\mathcal{Y}}} \max_{w \in {\mathcal{W}}} \sum_{x \in {\mathcal{X}}} \pi [x] \mathcal{C}_{\leak}[x, y] g(w, x, y) \\
= & EV_g(\pi, \mathcal{C}_{\leak}).
\end{align}

In the proof, equation \ref{thm-equiv-1} is just writing out the adversary explicitly. Equation \ref{thm-equiv-2} uses the fact that for any given leakage, there is a deterministic guess that maximises the conditional expectation and the best adversary overall does this for any leakage $\leak$ he sees. The next few lines of the proof are just re-writing the expectation into algebraic form and re-arranging the terms.
\end{proof}

In the next section, we will present a few constructions of searchable encryptions. In the section after, we will show how to use the extended posterior g-vulnerability and the idealised game with gain function to analyse security of the schemes.