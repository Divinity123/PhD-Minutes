In this section, we discuss why previous notions are not sufficient in defining security of searchable encryption schemes. We then motivates with a security notion for keyword guessing attack on searchable encrypted databases. We modify the experiment by considering a leakage function instead of the real encryption function. After that, we generalise the experiment to allow for arbitrary adversarial goals.



\subsection{Previous Notions}
In the literature, security notions for searchable encrypted databases comes in two flavors. The first type is based on an indistinguishability game which tries to capture the idea that the encrypted index does not leak any information about the underlying files. The first notion, known as indistinguishability under chosen keyword attack (IND-CKA) is proposed by Goh in \cite{EPRINT:Goh03}. The notion is shown to be insufficient so a stronger notion called IND2-CKA is proposed in the same paper. Most of the security notions in the literature are along this line. The main problem with this notion is that it is non-adaptive in nature. Curtmola et al. \cite{CCS:CGKO06} improved on the notion and proposed non-adaptive indistinguishability security and adaptive indistinguishability security for searchable symmetric encryption (SSE). The most remarkable difference between Curtmola's notion and Goh's is that the trace\footnote{Trace is defined to be the collection of document sizes, search pattern and access pattern.} of database is allowed to be leaked. This is exactly the second type of notions. The idea of allowing some leakage is then generalised to be beyond just the trace \cite{AC:ChaKam10, C:CJJKRS13, NDSS:StePapShi14,ESORICS:FJKNRS15}. 

However, security with leakage does not imply security in practice. One needs to understand what those leakages really reveals about the underlying database. In some cases, that is obvious. For example, if the leakage function is everything of the database then we know the encryption scheme is not secure, and if the leakage function reveals nothing then the scheme is secure. But with anything in between, we are not able to draw conclusion immediately. In fact, many types of leakages can be abused to recover information about the underlying database. If a scheme leaks repetition of keywords then it may be vulnerable to frequency-based statistical attack \cite{CCS:NavKamWri15}. If a scheme leaks co-occurrence of keywords then it can be exploited by IKK attack \cite{NDSS:IslKuzKan12} and count attack \cite{CCS:CGPR15}. Notably, co-occurrence is part of search pattern and access pattern and those are leaked by most of the schemes. More complicated leakage pattern involving bloom filter can also be misused \cite{CCS:PouWri16}. Therefore, in order to prove that a scheme is secure for practical use, we have to show that the leakage does not reveal information of the underlying database we try to hide.

The first step towards this goal is to consider a snapshot adversary who only has access to the encrypted database in the static manner and tries to recover some information. To make it more concrete, we consider an adversary who tries to recover some encrypted keywords. We call the security notion security against keyword guessing attack.




\subsection{Security Notion for Keyword Guessing Attack}
There is a multitude of ways to evaluate how well the adversary has recovered information through keyword guessing attack. We will present with one of the simplest here. Namely, the adversary wins if he recovers a correct pair of keyword and its encryption (with one guess).

For simplicity, we only consider snapshot attack here, so it suffices to assume that the searchable encryption scheme has some key generation function $\kgen(\cdot)$, some encryption function $\enc(\cdot)$ that encrypts the database, and there is a decryption function $\dec(\cdot)$ that decrypts encrypted documents one at a time. Because we are interested in security of the search index, we abuse $\dec(\cdot)$ to be the function that decrypts an encrypted document or a set of encrypted keywords associated to an encrypted document, and outputs the set of real keywords.

\begin{definition}[Security against Keyword Guessing Attack] \label{Security against Keyword Guessing Attack}
We define an keyword guessing attack adversary to be $\mathcal{A} = (\mathcal{A}_0, \mathcal{A}_1)$, where $\mathcal{A}_0$ is a probabilistic polynomial time (PPT) adversary which takes in the security parameter and generates a state $\state$ and a database $\db$, and $\mathcal{A}_1$ is a PPT adversary which takes in the security parameter, state $\state$ and encrypted database $\edb$, and returns a pair of keyword and encrypted keyword. The experiment can be described as follows:

\begin{pchstack}[center]
	\procedure{Exp$_{\adv}^{\text{Keyword-Guessing}}(n)$}{%
		\pcln \key \sample \kgen(\secparam) \\
		\pcln (\state, \db)  \gets \mathcal{A}_0(\secparam) \\
		\pcln \edb \gets \enc(\secparam, \key, \db) \\
		\pcln (m, c) \gets \mathcal{A}_1(\secparam, \state, \edb) \\
		\pcln \pcif (m \in \KWset(\db)) \wedge (c \in \EKWset(\edb)) \wedge (\dec(c) = m) \\
		\pcln \pcind \pcreturn 1 - f(m, \db) \\
		\pcln \pcelse \pcreturn - f(m, \db)
	}
\end{pchstack}
where $f(m, \db)$ is a function that returns the frequency of keyword $m$ in database $\db$.

Advantage $\advantage_{\mathcal{A}}^{\text{Keyword-Guessing}}(n)$ of an inference attack adversary $\mathcal{A}$ is defined to be
\begin{equation}
	\advantage_{\mathcal{A}}^{\text{Keyword-Guessing}}(n) = \expect{\text{Exp}_{\adv}^{\text{Keyword-Guessing}}(n)}.
\end{equation}
We say that a scheme is secure against keyword guessing attack if $\advantage_{\mathcal{A}}^{\text{Keyword-Guessing}}(n)$ is less than some negligible function in $n$.
\end{definition}


Intuitively, a scheme is secure against keyword guessing attack if there is no way he can guess any plaintext-ciphertext pair with probability greater than the frequency of the plaintexts. There are other possible definitions, for instance, one can look at the probability the adversary guessing all pairs of plaintexts and ciphertexts correctly.

This security notion is hard to work with because the database the adversary can generate is literally anything and the encryption scheme can generate any ciphertext. But in our game, we really do not care if keywords are `1' and `2' or `one' and `two', nor do we care how the ciphertexts look like. To simplify the problem, we want to work with idealised databases and encryption schemes which allows us to ignore information that are not relevant to our security game. Due to necessity of efficient searchability, encryption schemes for searchable encryption cannot achieve IND-CPA security, and the `insecure part' of encryption function (and query functions) are understood in terms of leakage functions. Informally speaking, leakage function associated to a scheme captures what is leaked about the unencrypted database by looking at its encryption. For example, number of documents is part of leakage for any scheme that does not pad fake documents because after enough queries, all documents will be returned at least once (unless part of the database is never going to be returned by any query, but that defeats the purpose of putting them there in the first place).

In the next experiment, the adversary is only given the leakage associated to the mechanism applied to the generated database. We argue that by choosing the leakage carefully, the advantage for the experiment will be the same as the previous one, up to an additive negligible term. We begin by defining leakage of searchable encryption.


\begin{definition}[Leakage of Encryption Schemes for Searching]
Let $\enc$ be the encryption algorithm for some searchable encryption scheme. We say $\leakm: \db \times \key \rightarrow \{0,1\}^{*}$ is a valid leakage function for the mechanism if for all adversaries $\mathcal{A}$ there exists a simulator $\mathcal{S}$ such that for any PPT distinguishers $\mathcal{D}$, the following two games are indistinguishable.
	
\begin{pchstack}[center]
\procedure{Real$_{\mathcal{A}}(n)$}{%
	\pcln \key \sample \kgen(\secparam) \\
	\pcln \db  \gets \mathcal{A}(\secparam) \\
	\pcln \edb \gets \enc(\secparam, \key, \db) \\
	\pcln \pcreturn \edb
}	

\pchspace
\procedure{Sim$_{\mathcal{A,S}}(n)$}{%
	\pcln \key  \sample \kgen(\secparam) \\
	\pcln \db   \gets \mathcal{A}(\secparam) \\
	\pcln \leak \gets \leakm(\secparam, \key, \db) \\
	\pcln \edb  \gets \mathcal{S}(\secparam, \leak) \\
	\pcln \pcreturn \edb
}
\end{pchstack}

$\leakm$ is said to be minimal if there exists a simulator $\mathcal{S}$ such that for all adversaries $\mathcal{A}$, no PPT distinguishers $\mathcal{D}$ can distinguish the two games above.
\end{definition}


Now we are ready to define security against idealised keyword guessing attack.


\begin{definition}[Security against Idealised Keyword Guessing Attack]
We define an idealised keyword guessing attack adversary to be $\mathcal{A} = (\mathcal{A}_0, \mathcal{A}_1)$, where $\mathcal{A}_0$ is a PPT adversary which takes in the security parameter and generates a state $\state$ and a database $\db$, and $\mathcal{A}_1$ is any adversary (not necessarily polynomial time bounded) which takes in the security parameter, state $\state$ and leakage $\leak$ generated by the minimal leakage function $\leakm$, and returns a pair of keyword and encrypted keyword. The experiment can be described as follows:
\begin{pchstack}[center]
	\procedure{Exp$_{\adv}^{\text{Keyword-Guessing-Ideal}}(n)$}{%
		\pcln \key \sample \kgen(\secparam) \\
		\pcln (\state, \db)  \gets \mathcal{A}_0(\secparam) \\
		\pcln \leak \gets \leakm(\secparam, \key, \db) \\
		\pcln (m, c) \gets \mathcal{A}_1(\secparam, \state, \edb) \\
		\pcln \pcif (m \in \KWset(\db)) \wedge (c \in \EKWset(\edb)) \wedge (\dec(c) = m) \\
		\pcln \pcind \pcreturn 1 - f(m, \db) \\
		\pcln \pcelse \pcreturn - f(m, \db)
	}
\end{pchstack}
where $f(m, \db)$ is a function that returns the frequency of keyword $m$ in database $\db$.

Advantage $\advantage_{\mathcal{A}}^{\text{Keyword-Guessing-Ideal}}(n)$ of an idealised inference attack adversary $\mathcal{A}$ is defined to be
\begin{equation}
\advantage_{\mathcal{A}}^{\text{Keyword-Guessing-Ideal}}(n) = \expect{\text{Exp}_{\adv}^{\text{Keyword-Guessing-Ideal}}(n)}.
\end{equation}
We say that a scheme is secure against idealised keyword guessing attack if $\advantage_{\mathcal{A}}^{\text{Keyword-Guessing-Ideal}}(n)$ is less than some negligible function in $n$.
\end{definition}

Notice that in the definition, $\mathcal{A}_1$ does not need to be PT bounded. This is because we are in the idealised world where the cryptographic part has already been dealt with in the leakage. Also, the decryption function $\dec(\cdot)$ we have in the experiment is idealised. For example, if an IND-DCPA secure deterministic encryption is used to encrypt keywords, then the idealised encryption function and decryption function are just bijections. This is different from original encryption and decryption functions because they are not keyed so it the adversary cannot use brute force to recover the key and break security of the scheme.

It is clear that if $\mathcal{L_M}$ is a well-defined minimal leakage, then all PPT distinguisher $\mathcal{D}$ can only distinguish the real experiment and the idealised experiment with probability no greater than a negligible function in $n$.



\subsection{Moving from Fixed Target Function to Generic Gain Function}
In the two games for keyword guessing attack we defined above, the content returned by the games are fixed so proving security with respect to the games only says the scheme is secure against this specific adversary who tries to recover this specific information. This is very inflexible as a security notion for application in searchable encryption. In addition, wrong guesses in these games are completely punished in this setting in the sense that they will never show up in the advantage term, even though wrong guesses can still reveal some information. Consider the case where the adversary always tries to guess decryption of some ciphertext $c$ and he always fails because his guess is some $m$ such that $\dec(c) \neq m$. But ultimately, the goal of the adversary can be to recover some keywords in some documents. If $m$ always appears together with $\dec(c)$ in all documents, then in fact, the adversary recovers a significant amount of information about the underlying database, even though the guess itself is wrong. So we need to reward those guesses that are not completely correct but still reveals information about the database appropriately.

To do so, we use idea of gain function from g-leakage paper \cite{6266165}. To put it in simple words, instead returning something from the experiment and analyse it in the advantage term, we compute the gain at the end of the experiment, and the advantage is defined to be the expectation of the result of the experiment for the best adversary. We now define the games with gain functions.


\begin{definition}[Real Game with Gain Function]
We define a real adversary with gain function $g$ to be $\mathcal{A} = (\mathcal{A}_0, \mathcal{A}_1)$, where $\mathcal{A}_0$ is a PPT adversary which takes in the security parameter and generates a database $\db$, and $\mathcal{A}_1$ is a PPT adversary which takes in the security parameter, generated database $\db$ and encrypted database $\edb$, and returns some guess $w$ in the set of allowed guesses $\mathcal{W}$. The experiment can be described as follows:
\begin{pchstack}[center]
\procedure{Real$_{\adv}^{\ g}(n)$}{%
	\pcln \key \sample \kgen(\secparam) \\
	\pcln \db  \gets \mathcal{A}_0(\secparam) \\
	\pcln \edb \gets \enc(\secparam, \key, \db) \\
	\pcln w \gets \mathcal{A}_1(\secparam, \db, \edb) \\
	\pcln \pcreturn g(w, (\db, \key), \edb)
}
\end{pchstack}

Advantage $\advantage_{\mathcal{A},g}^{\text{Real}}(n)$ is defined to be:
\begin{equation}
	\advantage_{\mathcal{A},g}^{\text{Real}}(n) = \expect{\text{Real}_{\adv}^{\ g}(n)}.
\end{equation}
\end{definition}


\begin{definition}[Ideal Game with Gain Function]
We define an ideal adversary with gain function $g$ to be $\mathcal{A} = (\mathcal{A}_0, \mathcal{A}_1)$, where $\mathcal{A}_0$ is a PPT adversary which takes in the security parameter and generates a database $\db$, and $\mathcal{A}_1$ is an adversary which takes in the security parameter, generated database $\db$ and a valid minimal leakage function $\leakm$, and returns some guess $w$ in the set of allowed guesses $\mathcal{W}$. The experiment can be described as follows:
\begin{pchstack}[center]
\procedure{Ideal$_{\adv}^{\ \mathcal{L}, g}(n)$}{%
	\pcln \key \sample \kgen(\secparam) \\
	\pcln \db  \gets \mathcal{A}_0(\secparam) \\
	\pcln \leak \gets \leakm(\secparam, \key, \db) \\
	\pcln w \gets \mathcal{A}_1(\secparam, \db, \leak) \\
	\pcln \pcreturn g(w, (\db, \key), \leak)
}
\end{pchstack}
	
Advantage $\advantage_{\mathcal{A}, \leakm, g}^{\text{Ideal}}(n)$ is defined to be:
\begin{equation}
	\advantage_{\mathcal{A}, \leakm, g}^{\text{Ideal}}(n) = \expect{\text{Ideal}_{\adv}^{\ \leakm, g}(n)}.
\end{equation}
\end{definition}

Note that we are not putting any condition to quantify when a scheme is secure under these notions, because the advantage really depends on the choice of $g$. But with well-defined $g$, the advantage will have an operational meaning and we can understand the level of security offered by our scheme from there.




\subsection{Games with Gain Function are equivalent to some Extended Posterior g-vulnerability}
In this subsection, we prove that in fact, we can look at the advantage from the games or some corresponding extended posterior g-vulnerability exchangeably. To do so, we need a slight tweak to the games above.

Let $\pi$ be the prior on the joint distribution of databases and keys. Then the games above is equivalent to the following.
\begin{pchstack}[center]
\procedure{Real$_{\adv}^{\ \pi,g}(n)$}{%
	\pcln (\db, \key) \sample \pi \\
	\pcln \edb \gets \enc(\secparam, \key, \db) \\
	\pcln w \gets \mathcal{A}_1(\secparam, \db, \edb) \\
	\pcln \pcreturn g(w, (\db, \key), \edb)
}

\pchspace
\procedure{Ideal$_{\adv}^{\ \pi, \leakm,g}(n)$}{%
	\pcln (\db, \key) \sample \pi \\
	\pcln \leak \gets \leakm(\secparam, \key, \db) \\
	\pcln w \gets \mathcal{A}_1(\secparam, \db, \leak) \\
	\pcln \pcreturn g(w, (\db, \key), \leak)
}
\end{pchstack}


\begin{theorem}[Ideal game is equivalent to an extended posterior g-vulnerability]
	Let $\pi$ of the extended posterior g-vulnerability be that in the real experiment. Let $\mathcal{X} = \{(\db, k)\}$. Let $\mathcal{Y} = \{\leakm(\secparam, \key, \db)\}$. Let $\channelm$ be the channel matrix from $\mathcal{X}$ to $\mathcal{Y}$. Then
	\begin{equation}
	\sup_{\pi \in \text{Dist}} \ \inf_{\leakm \in \text{Valid}} \ \sup_{\mathcal{A}} \advantage_{\mathcal{A}, \pi, \leakm, g}^{\text{Ideal}}(n) = EV_g(\pi, \channelm).
	\end{equation}
	where $Dist$ is the set of valid prior for the database. 
\end{theorem}


\begin{proof} 
For simplicity, we assume $Dist$ is a point distribution. Furthermore, $\inf_{\leakm \in \text{Valid}}$ is a technical condition to ensure the leakage we consider is really some valid leakage, so it suffices to look at the expression $\sup_{\mathcal{A}} \advantage_{\mathcal{A}, \pi, \leakm, g}^{\text{Real}}(n)$ with $\leakm$ being a valid leakage. Suppose $\leakm$ is indeed a valid leakage, then
\begin{align}
  & \sup_{\mathcal{A}} \advantage_{\mathcal{A}, \pi, \leakm, g}^{\text{Ideal}}(n) \\
= &	\sup_{\mathcal{A}} \expect{\text{Ideal}_{\adv}^{\ \leakm, g}(n)} \label{thm-equiv-1} \\
= & \mathbb{E}_{\leakm} \left[ \max_{w \in {\mathcal{W}}} \mathbb{E}_{\pi} \left[ g(w, (\db, \key), \leak \mid \leak \right] \right] \label{thm-equiv-2} \\
= &  \mathbb{E}_{\mathcal{Y}} \left[ \max_{w \in {\mathcal{W}}} \mathbb{E}_{\pi} \left[ g(w, \mathcal{X}, y) \mid y \right] \right] \\
= &  \sum_{y \in {\mathcal{Y}}} \prob{\mathcal{Y} = y} \max_{w \in {\mathcal{W}}} \sum_{x \in {\mathcal{X}}} \prob{\mathcal{X} = x \mid \mathcal{Y} = y} g(w, x, y) \\
= & \sum_{y \in {\mathcal{Y}}} \max_{w \in {\mathcal{W}}} \sum_{x \in {\mathcal{X}}} \prob{\mathcal{X} = x, \mathcal{Y} = y} g(w, x, y) \\
= & \sum_{y \in {\mathcal{Y}}} \max_{w \in {\mathcal{W}}} \sum_{x \in {\mathcal{X}}} \pi [x] \channelm[x, y] g(w, x, y) \\
= & EV_g(\pi, \channelm).
\end{align}

In the proof, equation \ref{thm-equiv-1} is just writing out the adversary explicitly. Equation \ref{thm-equiv-2} uses the fact that for any given leakage, there is a deterministic guess that maximises the conditional expectation and the best adversary overall does this for any leakage $\leak$ he sees. The next few lines of the proof are just re-writing the expectation into algebraic form and re-arranging the terms.
\end{proof}

In the next section, we will present a few constructions of searchable encryptions. In the section after, we will show how to use the extended posterior g-vulnerability and the idealised game with gain function to analyse security of the schemes.